{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2330, 5041, 371, 749, 146, 146, 146, 146, 138, 146, 146, 138, 146, 138, 10279, 2168, 2169, 1, 273, 740, 2819, 7814, 12834, 12042, 9026, 5787, 12834, 12042, 5820, 8901, 12867, 3416, 5835, 8562, 146, 146, 146, 138, 1570, 13355, 1071, 10482, 7296, 3875, 12834, 12042, 5820, 8901, 12867, 3416, 5835, 9560, 7315, 5704, 8624, 7154, 12834, 8624, 7154, 12834, 7404, 5837, 12946, 442, 442, 442, 442, 442, 442, 442, 6011, 12043, 12834, 8624, 9475, 5041, 77, 5212, 11846, 5820, 5964, 13611, 12834, 6501, 5835, 10482, 12834, 11846, 2800, 12834, 12040, 10505, 5041, 77, 12007, 12833, 13679, 8901, 12596, 6343, 10225, 8646, 12330, 9026, 12946, 12834, 7608, 9540, 12946, 12834, 11846, 1568, 13528, 6110, 10403, 8478, 5534, 12479, 8909, 12946, 10224, 12834, 11846, 10505, 12834, 7404, 8646, 12834, 11846, 12946, 12834, 7255, 9421, 10505, 12867, 9540, 12946, 6331, 8605, 1293, 7742, 9173, 1697, 656, 12867, 9540, 7829, 6558, 7137, 5704, 8606, 12708, 9173, 12834, 11846, 5212, 8606, 12708, 12788, 12834, 11846, 12833, 12834, 11846, 9421, 442, 442, 434, 5225, 9421, 12842, 10731, 12834, 7404, 12946, 9572, 5820, 13566, 8051, 12834, 7404, 9173, 5704, 10345, 8624, 6011, 12043, 12834, 8624, 9475, 9572, 7608, 3416, 5835, 10482, 9421, 10482, 12834, 8624, 9475, 5041, 77, 5212, 7404, 9540, 9216, 12834, 3416, 5835, 10482, 12834, 13185, 7608, 1224, 9046, 7814, 12834, 11846, 10461, 12867, 7608, 3416, 5835, 2512, 1071, 10482, 7296, 1071, 8562, 2299, 9540, 7480, 9173, 4393, 961, 1017, 10354, 9486, 12946, 1071, 9540, 8757, 9173, 12834, 5103, 2891, 13154, 4393, 165, 5534, 9833, 8314]\n",
      "tensor([[ 2330,  5041,   371,   749,   146,   146,   146,   146,   138,   146,\n",
      "           146,   138,   146,   138, 10279,  2168,  2169,     1,   273,   740,\n",
      "          2819,  7814, 12834, 12042,  9026,  5787, 12834, 12042,  5820,  8901,\n",
      "         12867,  3416,  5835,  8562,   146,   146,   146,   138,  1570, 13355,\n",
      "          1071, 10482,  7296,  3875, 12834, 12042,  5820,  8901, 12867,  3416,\n",
      "          5835,  9560,  7315,  5704,  8624,  7154, 12834,  8624,  7154, 12834,\n",
      "          7404,  5837, 12946,   442,   442,   442,   442,   442,   442,   442,\n",
      "          6011, 12043, 12834,  8624,  9475,  5041,    77,  5212, 11846,  5820,\n",
      "          5964, 13611, 12834,  6501,  5835, 10482, 12834, 11846,  2800, 12834,\n",
      "         12040, 10505,  5041,    77, 12007, 12833, 13679,  8901, 12596,  6343,\n",
      "         10225,  8646, 12330,  9026, 12946, 12834,  7608,  9540, 12946, 12834,\n",
      "         11846,  1568, 13528,  6110, 10403,  8478,  5534, 12479,  8909, 12946,\n",
      "         10224, 12834, 11846, 10505, 12834,  7404,  8646, 12834, 11846, 12946,\n",
      "         12834,  7255,  9421, 10505, 12867,  9540, 12946,  6331,  8605,  1293,\n",
      "          7742,  9173,  1697,   656, 12867,  9540,  7829,  6558,  7137,  5704,\n",
      "          8606, 12708,  9173, 12834, 11846,  5212,  8606, 12708, 12788, 12834,\n",
      "         11846, 12833, 12834, 11846,  9421,   442,   442,   434,  5225,  9421,\n",
      "         12842, 10731, 12834,  7404, 12946,  9572,  5820, 13566,  8051, 12834,\n",
      "          7404,  9173,  5704, 10345,  8624,  6011, 12043, 12834,  8624,  9475,\n",
      "          9572,  7608,  3416,  5835, 10482,  9421, 10482, 12834,  8624,  9475,\n",
      "          5041,    77,  5212,  7404,  9540,  9216, 12834,  3416,  5835, 10482,\n",
      "         12834, 13185,  7608,  1224,  9046,  7814, 12834, 11846, 10461, 12867,\n",
      "          7608,  3416,  5835,  2512,  1071, 10482,  7296,  1071,  8562,  2299,\n",
      "          9540,  7480,  9173,  4393,   961,  1017, 10354,  9486, 12946,  1071,\n",
      "          9540,  8757,  9173, 12834,  5103,  2891, 13154,  4393,   165,  5534,\n",
      "          9833,  8314]])\n",
      "tensor([ 2330,  5041,   371,   749,   146,   146,   146,   146,   138,   146,\n",
      "          146,   138,   146,   138, 10279,  2168,  2169,     1,   273,   740,\n",
      "         2819,  7814, 12834, 12042,  9026,  5787, 12834, 12042,  5820,  8901,\n",
      "        12867,  3416,  5835,  8562,   146,   146,   146,   138,  1570, 13355,\n",
      "         1071, 10482,  7296,  3875, 12834, 12042,  5820,  8901, 12867,  3416,\n",
      "         5835,  9560,  7315,  5704,  8624,  7154, 12834,  8624,  7154, 12834,\n",
      "         7404,  5837, 12946,   442,   442,   442,   442,   442,   442,   442,\n",
      "         6011, 12043, 12834,  8624,  9475,  5041,    77,  5212, 11846,  5820,\n",
      "         5964, 13611, 12834,  6501,  5835, 10482, 12834, 11846,  2800, 12834,\n",
      "        12040, 10505,  5041,    77, 12007, 12833, 13679,  8901, 12596,  6343,\n",
      "        10225,  8646, 12330,  9026, 12946, 12834,  7608,  9540, 12946, 12834,\n",
      "        11846,  1568, 13528,  6110, 10403,  8478,  5534, 12479,  8909, 12946,\n",
      "        10224, 12834, 11846, 10505, 12834,  7404,  8646, 12834, 11846, 12946,\n",
      "        12834,  7255,  9421, 10505, 12867,  9540, 12946,  6331,  8605,  1293,\n",
      "         7742,  9173,  1697,   656, 12867,  9540,  7829,  6558,  7137,  5704,\n",
      "         8606, 12708,  9173, 12834, 11846,  5212,  8606, 12708, 12788, 12834,\n",
      "        11846, 12833, 12834, 11846,  9421,   442,   442,   434,  5225,  9421,\n",
      "        12842, 10731, 12834,  7404, 12946,  9572,  5820, 13566,  8051, 12834,\n",
      "         7404,  9173,  5704, 10345,  8624,  6011, 12043, 12834,  8624,  9475,\n",
      "         9572,  7608,  3416,  5835, 10482,  9421, 10482, 12834,  8624,  9475,\n",
      "         5041,    77,  5212,  7404,  9540,  9216, 12834,  3416,  5835, 10482,\n",
      "        12834, 13185,  7608,  1224,  9046,  7814, 12834, 11846, 10461, 12867,\n",
      "         7608,  3416,  5835,  2512,  1071, 10482,  7296,  1071,  8562,  2299,\n",
      "         9540,  7480,  9173,  4393,   961,  1017, 10354,  9486, 12946,  1071,\n",
      "         9540,  8757,  9173, 12834,  5103,  2891, 13154,  4393,   165,  5534,\n",
      "         9833,  8314])\n",
      "Explain Subnet 2 5 111 111 111 111 110 111 111 110 111 110 namely E6 E9 00 17BB 4B How does the sending host acquire the sending adapter has this MAC address for 111 111 111 110 By using ARP of course Once the sending adapter has this MAC address it creates a frame containing the frame containing the datagram addressed to 222 222 222 222 222 222 222 and sends the frame into Subnet 1 The router adapter along with the broadcast address of the router Hooray the sender on Subnet 1 sees that x has successfully been moved from source host to the destination is to the router But we are not finished We still have to move the router on the datagram from the router to the correct interface on this is to be forwarded As discussed in Chapter 4 this is done by consulting a forwarding table in the router The forwarding table tells the router that the router interface 222 222 220 This interface then passes the datagram to its adapter which encapsulates the datagram in a new frame and sends the frame into its destination MAC address of interface of the frame into Subnet 1 The datagram is indeed the MAC address of the ultimate destination And how does the router obtain this destination MAC address From ARP of course ARP for Ethernet is defined in RFC 826 A nice introduction to ARP is given in the TCP IP tutorial RFC 1180 We ll explore\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ccn_model import GPTLanguageModel, encode, decode\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Replace 'path/to/your/model.pth' with the actual path where you saved your trained model\n",
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load('./ccn.pth', map_location=torch.device('cpu')))\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode for generation\n",
    "\n",
    "# Prepare input context\n",
    "input_context = \"Explain Subnet\"\n",
    "\n",
    "# Encode the input context into integers representing tokens\n",
    "encoded_context = encode(input_context)\n",
    "\n",
    "# Convert the encoded context into a torch tensor\n",
    "context_tensor = torch.tensor([encoded_context], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate text based on the input context\n",
    "max_new_tokens = 250  # Specify the maximum number of tokens to generate\n",
    "generated_sequence = model.generate(context_tensor, max_new_tokens)\n",
    "\n",
    "# Decode the generated sequence of tokens back into text\n",
    "generated_text = decode(generated_sequence[0].tolist())\n",
    "\n",
    "print(generated_sequence[0].tolist())\n",
    "print(generated_sequence)\n",
    "print(generated_sequence[0])\n",
    "print(decode(generated_sequence[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
